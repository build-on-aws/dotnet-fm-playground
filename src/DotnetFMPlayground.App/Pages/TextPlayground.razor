@page "/text-playground"
@using System.Text;
@using Amazon.BedrockRuntime;
@using Amazon.Bedrock;
@using Amazon.Bedrock.Model;
@using Rockhead.Extensions;
@using System.Text.Json;
@using DotnetFMPlayground.Core.Models;
@using Rockhead.Extensions.Amazon;
@using Rockhead.Extensions.Anthropic;
@using Rockhead.Extensions.AI21Labs;
@using Rockhead.Extensions.Cohere;
@using Rockhead.Extensions.Meta;
@using Rockhead.Extensions.MistralAI;
@using DotnetFMPlayground.App.Components;
@inject AmazonBedrockRuntimeClient BedrockRuntimeClient
@inject AmazonBedrockClient BedrockClient
@inject IJSRuntime JS

<MudText Typo="Typo.h3">Text Playground</MudText>
<MudStack>
    <MudPaper Class="pa-2">
        <MudSelect Class="ma-2" T="FoundationModelSummary" Value="@selectedModel" ValueChanged="@OnSelectedModelChanged" ToStringFunc="@selectConverter" Required="true">
                @if (foundationModels != null)
                {
                    foreach (var item in foundationModels)
                    {
                        <MudSelectItem Value="@item" />
                    }
                }
        </MudSelect>
        <MudStack Class="ma-2" Row="true">
            @if (PromptFormatAvailable())
            {
                <MudButton Class="ma-2" Variant="Variant.Filled" StartIcon="@Icons.Material.Filled.Article" Color="Color.Primary" OnClick="OnAddTemplate">Prompt Format Template</MudButton>
            }
            @if (MessagesAPIAvailable())
            {
                <MudSwitch Class="ma-2" @bind-Checked="_messagesAPI" Label="Messages API" T="bool" Color="Color.Primary"></MudSwitch>
            }
            @if (_model != null && _model.StreamingSupported)
            {
                <MudSwitch Class="ma-2" Checked="@_streaming" Label="Reponse streaming" T="bool" CheckedChanged="@OnCheckedChanged" Color="Color.Primary"></MudSwitch>
            }
        </MudStack>
        @if (_model != null && _model is Model.Claude)
        {
            <ClaudeInferenceParameters @ref="_inferenceParametersBase" Class="ma-2"></ClaudeInferenceParameters>
        }
        else if(_model != null && _model is Model.TitanText)
        {
            <TitanTextInferenceParameters @ref="_inferenceParametersBase" Class="ma-2"></TitanTextInferenceParameters>
        }
        else if(_model != null && _model is Model.Jurassic2)
        {
            <Jurassic2TextInferenceParameters @ref="_inferenceParametersBase" Class="ma-2"></Jurassic2TextInferenceParameters>
        }
        else if(_model != null && _model is Model.CommandText)
        {
            <CommandTextInferenceParameters @ref="_inferenceParametersBase" Class="ma-2"></CommandTextInferenceParameters>
        }
        else if (_model != null && _model is Model.Llama)
        {
            <LlamaTextInferenceParameters @ref="_inferenceParametersBase" Class="ma-2"></LlamaTextInferenceParameters>
        }
        else if(_model != null && _model is Model.Mistral)
        {
            <MistralTextInferenceParameters @ref="_inferenceParametersBase" Class="ma-2"></MistralTextInferenceParameters>
        }
    </MudPaper>
    <EditForm Model="@userPrompt" OnSubmit="OnSubmit">
        <MudCard>
            <MudCardContent>
                <MudTextField Counter=0 id="PromptId" Label="Prompt" @bind-Value="userPrompt.Prompt" Lines=5 Variant="Variant.Outlined" />
            </MudCardContent>
            <MudCardActions>
                <MudButton ButtonType="ButtonType.Submit" Variant="Variant.Filled" Color="Color.Primary" Class="ml-auto">Submit</MudButton>
            </MudCardActions>
        </MudCard>
    </EditForm>
    <MudCard>
        <MudCardContent>
            <MudField Id="ResponseField" Label="Response" Class="white-space-pre-line">@outputText</MudField>
        </MudCardContent>
    </MudCard>
</MudStack>
@code {

    public class UserPrompt
    {
        public string Prompt { get; set; }
    }

    private IEnumerable<FoundationModelSummary> foundationModels;

    private FoundationModelSummary selectedModel;

    private Model _model;

    private UserPrompt userPrompt = new UserPrompt();

    private string outputText;

    Func<FoundationModelSummary, string> selectConverter = fms => fms == null ? "" : String.Concat(fms?.ModelName, " (", fms?.ModelId, ")");

    private bool _streaming = false;

    private bool _messagesAPI = false;

    private InferenceParametersBase? _inferenceParametersBase = null;

    protected override async Task OnInitializedAsync()
    {

        foundationModels = (await BedrockClient.ListFoundationModelsAsync(new ListFoundationModelsRequest())).ModelSummaries.Where(x => x.OutputModalities.Contains("TEXT") && Model.IsSupported(x.ModelId));
        selectedModel = foundationModels.FirstOrDefault();
        UpdateModel();
        await base.OnInitializedAsync();
    }

    private void OnSelectedModelChanged(FoundationModelSummary model)
    {
        selectedModel = model;
        UpdateModel();
        outputText = string.Empty;
    }

    private void UpdateModel()
    {
        _model = Model.Parse(selectedModel.ModelId);
        _streaming = false;
        _messagesAPI = false;
    }

    private void OnCheckedChanged(bool value)
    {
        _streaming = value;
    }

    private void OnAddTemplate(MouseEventArgs evt)
    {
        userPrompt.Prompt = GetPromptFormat();
    }

    private const string _claudeTextGenerationTemplate = "Human: {{user_message}}\n\nAssistant:";
    private const string _llama2ChatPromptTemplate = "<s>\n[INST]\n<<SYS>>\n{{ system_prompt }}\n<</SYS>>\n{{ user_message }}\n[/INST]";
    private const string _llama3InstructPromptTemplate = "<|begin_of_text|><|start_header_id|>system<|end_header_id|>{{ system_prompt }}<|eot_id|>\n<|start_header_id|>user<|end_header_id|>{{ user_message }}<|eot_id|>\n<|start_header_id|>assistant<|end_header_id|>";
    private const string _mistralInstructTemplate = "<s>[INST]{{ user_message }}[/INST]";
    private string GetPromptFormat() =>
        _model switch
        {
            Model.ClaudeTextCompletionSupport => _claudeTextGenerationTemplate,
            Model.Llama213BChatV1 => _llama2ChatPromptTemplate,
            Model.Llama270BChatV1 => _llama2ChatPromptTemplate,
            Model.Llama38BInstructV1 => _llama3InstructPromptTemplate,
            Model.Llama370BInstructV1 => _llama3InstructPromptTemplate,
            Model.Mistral => _mistralInstructTemplate,
            _ => string.Empty
        };

    private bool PromptFormatAvailable() =>
        _model switch
        {
            Model.ClaudeTextCompletionSupport => true,
            Model.Llama213BChatV1 => true,
            Model.Llama270BChatV1 => true,
            Model.Llama38BInstructV1 => true,
            Model.Llama370BInstructV1 => true,
            Model.Mistral => true,
            _ => false
        };

    private bool MessagesAPIAvailable() =>
    _model switch
    {
        Model.ClaudeTextCompletionSupport => true,
        _ => false
    };

    private async Task OnSubmit(EditContext context)
    {
        outputText = string.Empty;
        Prompt prompt = new();
        prompt.Add(new PromptItem(PromptItemType.User, userPrompt.Prompt));

        try
        {
            if(_model.StreamingSupported && _streaming)
            {
                await foreach(var chunk in InvokeModelWithStreamingAsync(prompt))
                {
                    outputText += chunk;
                    StateHasChanged();
                    await JS.InvokeVoidAsync("scrollToElement", "ResponseField");
                }
            }
            else
            {
                outputText = await InvokeModelAsync(prompt);
                StateHasChanged();
                await JS.InvokeVoidAsync("scrollToElement", "ResponseField");
            }
        }
        catch (Exception e)
        {
            outputText = e.Message;
            StateHasChanged();
        }
    }

    private async Task<string?> InvokeModelAsync(Prompt prompt)
    {
        Model model = Model.Parse(selectedModel.ModelId);
        IFoundationModelResponse? response = null;
        string textPrompt = new StringBuilder().AppendJoin(' ', prompt.Select(x => x.Prompt)).ToString();
        if (model is Model.TitanText)
        {
            TitanTextGenerationConfig textGenerationConfig = null;
            if(_inferenceParametersBase != null)
            {
                textGenerationConfig = GetTitanTextGenerationConfig();
            }
            response = await BedrockRuntimeClient.InvokeTitanTextG1Async((Model.TitanText)model, textPrompt, textGenerationConfig);
        }
        else if(model is Model.Jurassic2)
        {
            Jurassic2TextGenerationConfig textGenerationConfig = null;
            if (_inferenceParametersBase != null)
            {
                textGenerationConfig = GetJurassic2TextGenerationConfig();
            }
            response = await BedrockRuntimeClient.InvokeJurassic2Async((Model.Jurassic2)model, textPrompt, textGenerationConfig);
        }
        else if(model is Model.ClaudeTextCompletionSupport && !_messagesAPI)
        {
            ClaudeTextGenerationConfig? textGenerationConfig = null;
            if(_inferenceParametersBase is not null)
            {
                textGenerationConfig = GetClaudeTextGenerationConfig();
            }
            response = await BedrockRuntimeClient.InvokeClaudeAsync((Model.Claude)model, textPrompt, textGenerationConfig);
        }
        else if(model is Model.Claude)
        {
            ClaudeMessage message = new ClaudeMessage() { Role = "user", Content = new[] { new ClaudeTextContent() { Text = textPrompt } } };
            ClaudeMessagesConfig? messagesConfig = null;
            if (_inferenceParametersBase is not null)
            {
                messagesConfig = GetClaudeMessagesConfig();
            }
            response = await BedrockRuntimeClient.InvokeClaudeMessagesAsync((Model.Claude)model, message, messagesConfig);
        }
        else if(model is Model.CommandText)
        {
            CommandTextGenerationConfig? textGenerationConfig = null;
            if (_inferenceParametersBase is not null)
            {
                textGenerationConfig = GetCommandTextGenerationConfig();
            }
            response = await BedrockRuntimeClient.InvokeCommandV14Async((Model.CommandText)model, textPrompt, textGenerationConfig);
        }
        else if(model is Model.Llama)
        {
            LlamaTextGenerationConfig? textGenerationConfig = null;
            if (_inferenceParametersBase is not null)
            {
                textGenerationConfig = GetLlamaTextGenerationConfig();
            }
            response = await BedrockRuntimeClient.InvokeLlamaAsync((Model.Llama)model, textPrompt, textGenerationConfig);
        }
        else if(model is Model.Mistral)
        {
            MistralTextGenerationConfig? textGenerationConfig = null;
            if (_inferenceParametersBase is not null)
            {
                textGenerationConfig = GetMistralTextGenerationConfig();
            }
            response = await BedrockRuntimeClient.InvokeMistralAsync((Model.Mistral)model, textPrompt, textGenerationConfig);
        }
        return response?.GetResponse();
    }

    private async IAsyncEnumerable<string?> InvokeModelWithStreamingAsync(Prompt prompt)
    {
        Model model = Model.Parse(selectedModel.ModelId);
        string textPrompt = new StringBuilder().AppendJoin(' ', prompt.Select(x => x.Prompt)).ToString();
        if (model is Model.TitanText)
        {
            TitanTextGenerationConfig textGenerationConfig = null;
            if (_inferenceParametersBase != null)
            {
                textGenerationConfig = GetTitanTextGenerationConfig();
            }
            await foreach (var chunk in BedrockRuntimeClient.InvokeTitanTextG1WithResponseStreamAsync((Model.TitanText)model, textPrompt, textGenerationConfig))
            {
                yield return chunk.GetResponse();
            }
        }
        else if (model is Model.ClaudeTextCompletionSupport && !_messagesAPI)
        {
            ClaudeTextGenerationConfig? textGenerationConfig = null;
            if (_inferenceParametersBase is not null)
            {
                textGenerationConfig = GetClaudeTextGenerationConfig();
            }
            await foreach (var chunk in BedrockRuntimeClient.InvokeClaudeWithResponseStreamAsync((Model.Claude)model, textPrompt, textGenerationConfig))
            {
                yield return chunk.GetResponse();
            }
        }
        else if (model is Model.Claude)
        {
            ClaudeMessage message = new ClaudeMessage() { Role = "user", Content = new[] { new ClaudeTextContent() { Text = textPrompt } } };
            ClaudeMessagesConfig? messagesConfig = null;
            if (_inferenceParametersBase is not null)
            {
                messagesConfig = GetClaudeMessagesConfig();
            }
            await foreach (var chunk in BedrockRuntimeClient.InvokeClaudeMessagesWithResponseStreamAsync((Model.Claude)model, message, messagesConfig))
            {
                yield return chunk.GetResponse();
            }
        }
        else if (model is Model.CommandText)
        {
            CommandTextGenerationConfig? textGenerationConfig = null;
            if (_inferenceParametersBase is not null)
            {
                textGenerationConfig = GetCommandTextGenerationConfig();
            }
            await foreach (var chunk in BedrockRuntimeClient.InvokeCommandV14WithResponseStreamAsync((Model.CommandText)model, textPrompt, textGenerationConfig))
            {
                yield return chunk.GetResponse();
            }
        }
        else if (model is Model.Llama)
        {
            LlamaTextGenerationConfig? textGenerationConfig = null;
            if (_inferenceParametersBase is not null)
            {
                textGenerationConfig = GetLlamaTextGenerationConfig();
            }
            await foreach (var chunk in BedrockRuntimeClient.InvokeLlamaWithResponseStreamAsync((Model.Llama)model, textPrompt, textGenerationConfig))
            {
                yield return chunk.GetResponse();
            }
        }
        else if (model is Model.Mistral)
        {
            MistralTextGenerationConfig? textGenerationConfig = null;
            if (_inferenceParametersBase is not null)
            {
                textGenerationConfig = GetMistralTextGenerationConfig();
            }
            await foreach (var chunk in BedrockRuntimeClient.InvokeMistralWithResponseStreamAsync((Model.Mistral)model, textPrompt, textGenerationConfig))
            {
                yield return chunk.GetResponse();
            }
        }
    }

    private ClaudeTextGenerationConfig GetClaudeTextGenerationConfig()
    {
        float? temperature = null;
        float? topP = null;
        int? topK = null;
        int maxTokensToSample = 200;
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("temperature"))
        {
            temperature = (float)_inferenceParametersBase.InferenceParameters["temperature"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("top_p"))
        {
            topP = (float)_inferenceParametersBase.InferenceParameters["top_p"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("top_k"))
        {
            topK = (int)_inferenceParametersBase.InferenceParameters["top_k"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("max_tokens_to_sample"))
        {
            maxTokensToSample = (int)_inferenceParametersBase.InferenceParameters["max_tokens_to_sample"];
        }
        return new ClaudeTextGenerationConfig()
        {
            Temperature = temperature,
            TopP = topP,
            TopK = topK,
            MaxTokensToSample = maxTokensToSample
        };
    }

    private ClaudeMessagesConfig GetClaudeMessagesConfig()
    {
        float? temperature = null;
        float? topP = null;
        int? topK = null;
        int maxTokensToSample = 200;
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("temperature"))
        {
            temperature = (float)_inferenceParametersBase.InferenceParameters["temperature"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("top_p"))
        {
            topP = (float)_inferenceParametersBase.InferenceParameters["top_p"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("top_k"))
        {
            topK = (int)_inferenceParametersBase.InferenceParameters["top_k"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("max_tokens_to_sample"))
        {
            maxTokensToSample = (int)_inferenceParametersBase.InferenceParameters["max_tokens_to_sample"];
        }
        return new ClaudeMessagesConfig()
            {
                Temperature = temperature,
                TopP = topP,
                TopK = topK,
                MaxTokens = maxTokensToSample
            };
    }

    private TitanTextGenerationConfig GetTitanTextGenerationConfig()
    {
        float? temperature = null;
        float? topP = null;
        int maxTokensToSample = 512;
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("temperature"))
        {
            temperature = (float)_inferenceParametersBase.InferenceParameters["temperature"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("topP"))
        {
            topP = (float)_inferenceParametersBase.InferenceParameters["topP"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("maxTokenCount"))
        {
            maxTokensToSample = (int)_inferenceParametersBase.InferenceParameters["maxTokenCount"];
        }
        return new TitanTextGenerationConfig()
        {
            Temperature = temperature ?? 0,
            TopP = topP ?? 1,
            MaxTokenCount = maxTokensToSample
        };
    }

    private Jurassic2TextGenerationConfig GetJurassic2TextGenerationConfig()
    {
        float? temperature = null;
        float? topP = null;
        int maxTokensToSample = 512;
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("temperature"))
        {
            temperature = (float)_inferenceParametersBase.InferenceParameters["temperature"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("topP"))
        {
            topP = (float)_inferenceParametersBase.InferenceParameters["topP"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("maxTokenCount"))
        {
            maxTokensToSample = (int)_inferenceParametersBase.InferenceParameters["maxTokenCount"];
        }
        return new Jurassic2TextGenerationConfig()
            {
                Temperature = temperature ?? 0,
                TopP = topP ?? 1,
                MaxTokens = maxTokensToSample
            };
    }

    private CommandTextGenerationConfig GetCommandTextGenerationConfig()
    {
        float? temperature = null;
        float? topP = null;
        int? topK = null;
        int? maxTokensToSample = null;
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("temperature"))
        {
            temperature = (float)_inferenceParametersBase.InferenceParameters["temperature"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("top_p"))
        {
            topP = (float)_inferenceParametersBase.InferenceParameters["top_p"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("top_k"))
        {
            topK = (int)_inferenceParametersBase.InferenceParameters["top_k"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("max_tokens_to_sample"))
        {
            maxTokensToSample = (int)_inferenceParametersBase.InferenceParameters["max_tokens_to_sample"];
        }
        return new CommandTextGenerationConfig()
            {
                Temperature = temperature,
                P = topP,
                K = topK,
                MaxTokens = maxTokensToSample
            };
    }

    private LlamaTextGenerationConfig GetLlamaTextGenerationConfig()
    {
        float? temperature = null;
        float? topP = null;
        int maxTokensToSample = 200;
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("temperature"))
        {
            temperature = (float)_inferenceParametersBase.InferenceParameters["temperature"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("topP"))
        {
            topP = (float)_inferenceParametersBase.InferenceParameters["topP"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("maxTokenCount"))
        {
            maxTokensToSample = (int)_inferenceParametersBase.InferenceParameters["maxTokenCount"];
        }
        return new LlamaTextGenerationConfig()
            {
                Temperature = temperature ?? 0,
                TopP = topP ?? 1,
                MaxGenLen = maxTokensToSample
            };
    }

    private MistralTextGenerationConfig GetMistralTextGenerationConfig()
    {
        float? temperature = null;
        float? topP = null;
        int? topK = null;
        int? maxTokensToSample = null;
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("temperature"))
        {
            temperature = (float)_inferenceParametersBase.InferenceParameters["temperature"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("top_p"))
        {
            topP = (float)_inferenceParametersBase.InferenceParameters["top_p"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("top_k"))
        {
            topK = (int)_inferenceParametersBase.InferenceParameters["top_k"];
        }
        if (_inferenceParametersBase.InferenceParameters.ContainsKey("max_tokens_to_sample"))
        {
            maxTokensToSample = (int)_inferenceParametersBase.InferenceParameters["max_tokens_to_sample"];
        }
        return new MistralTextGenerationConfig()
            {
                Temperature = temperature,
                TopP = topP,
                TopK = topK,
                MaxTokens = maxTokensToSample
            };
    }
}

